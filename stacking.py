#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Nov 26 17:16:22 2020

@author: dulab19
"""

from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.linear_model import LogisticRegression as lr
from sklearn.model_selection import GridSearchCV
import numpy as np
from sklearn.feature_selection import RFECV
from sklearn.metrics import accuracy_score, make_scorer
import copy



class StackingModel():

    def __init__(self, estimators=None, feature_inds=None):
        """
        
        Parameters
        ----------
        estimators : list, optional
            list of estimators with params. The default is None.
        feature_inds : list, optional
            list of feature indexs(numpy array bool) of each estimator. The default is None.

        Returns
        -------
        None.

        """
        self.estimators = estimators
        self.feature_inds = feature_inds
        self.meta_estimator = lr(max_iter=900, solver='sag', random_state=2001)
        self.fited = [[] for _ in range(10)]
        # stores train set generated by cross_val for logistic regrssion
        self.train = None
        # stores test set generated by cross_val for logistic regression
        self.test = None
        self.model_metric = np.zeros((10, 3))
        self.meta_feature_inds = None
        self.class_num = 6

    def generate_train_test(self, x_train, y_train, num_class=6):
        """
        generate train and test set for meta estimator.

        Parameters
        ----------
        x_train : numpy array
            train data set.
        y_train : numpy array
            train labels.
        x_test : numpy array
            test data set.
        y_test : numpy array
            test labels.

        Returns
        -------
        None.

        """
        fold = 10
        n_estimators = len(self.estimators)
        # initialize stacking train test
        # self.test = np.zeros((x_test.shape[0], n_estimators*num_class))
        self.train = np.zeros((x_train.shape[0], n_estimators*num_class))
        # StratifiedKFold 
        k = 0
        skf = StratifiedKFold(n_splits = fold, shuffle=True, random_state=2021).split(x_train, y_train)# shuffle=True
        for train_ind, test_ind in skf:
            sub_x_train, sub_x_test = x_train[train_ind], x_train[test_ind]
            sub_y_train, sub_y_test = y_train[train_ind], y_train[test_ind]
            sub_trains = [sub_x_train[:, ind] for ind in self.feature_inds]
            sub_tests = [sub_x_test[:, ind] for ind in self.feature_inds]
            # x_tests = [x_test[:, ind] for ind in self.feature_inds]
            sub_probas = []
            # x_test_probas = []
            scores = []
            # fit estimators with subtrain
            for i in range(n_estimators):
                self.estimators[i].fit(sub_trains[i], sub_y_train)
                sub_probas.append(self.estimators[i].predict_proba(sub_tests[i]))
                score = self.estimators[i].score(sub_tests[i], sub_y_test)
                self.model_metric[k, i] = score
                # x_test_probas.append(self.estimators[i].predict_proba(x_tests[i]))
                scores.append(score)
                self.fited[k].append(copy.deepcopy(self.estimators[i]))
            tmp = sub_probas.pop(0)
            while sub_probas != []:
                tmp = np.concatenate((tmp,sub_probas.pop(0)), axis = 1)
            # store subtest predicted results as the trainset of logistic regression
            for ind, gp in zip(test_ind, tmp):
                self.train[ind] = np.array(gp)
            print("round {}:".format(k+1), scores)
            k += 1

    def adjust_param_lr(self, y_train):
        """
        search 'C' value with GridSearchCV for meta estimator.

        Parameters
        ----------
        y_train : TYPE
            DESCRIPTION.

        Returns
        -------
        None.

        """
        model = lr(max_iter = 2000, tol=0.005, solver = 'sag', random_state = 2021)
        cv_params = {'C': np.arange(0.9, 1.2, 0.01), 'tol':np.arange(0.001, 0.02, 0.001)}
        GS = GridSearchCV(model, cv_params, scoring = "f1_macro", verbose=1, cv=20, n_jobs = -1, return_train_score=True)
        GS.fit(self.train, y_train)
        best_param = GS.best_params_
        best_score = GS.best_score_
        print("best value:", best_param, " accuracy:", best_score)
        C_value = best_param['C']
        self.meta_estimator.set_params(**{'C': C_value})
        tol_value = best_param['tol']
        self.meta_estimator.set_params(**{'tol': tol_value})
    
    def weight_predict(self, x, estimator_ind):
        fold = 10
        x_tmp = np.zeros((x.shape[0], self.class_num))
        x_test = x[:, self.feature_inds[estimator_ind]]
        for k in range(fold):
            estimator = self.fited[k][estimator_ind]
            x_test_probas = estimator.predict_proba(x_test)*self.model_metric[k, estimator_ind]
            x_tmp += x_test_probas
        x_tmp /= sum(self.model_metric[:, estimator_ind])
        return x_tmp
            

        
    def predict(self, x, y_train, flag=True):
        """
        return prediction of test set

        Parameters
        ----------
        y_train : numpy array
            train labels.

        Raises
        ------
        ValueError
            When self.test is None, raise ValueError("Test dataset doesn't exist. Call 'generate_train_test' before using this function.").

        Returns
        -------
        y_test_p : numpy array
            prediction of test set.

        """
        n_estimators = len(self.estimators)
        fold = 10
        x_test = self.weight_predict(x, 0)
        # for k in range(fold):
        #     x_test_probas = []
        #     x_tests = [x[:, ind] for ind in self.feature_inds]
        #     for j in range(3):
        #         estimator = self.fited[k][j]
        #         x_test_probas.append(estimator.predict_proba(x_tests[j]))
        #     tmp = x_test_probas.pop(0)
        #     while x_test_probas != []:
        #         tmp = np.concatenate((tmp, x_test_probas.pop(0)), axis = 1)
        #     self.test += np.array(tmp)
        # self.test = self.test/fold
        for i in range(1, 3):
            tmp = self.weight_predict(x, i)
            x_test = np.concatenate((x_test, tmp), axis = 1)
        self.test = copy.deepcopy(x_test)
        if flag:
            # best_score = 0
            # for i in range(1, self.train.shape[1]):
            #     selector = RFECV(self.meta_estimator, step=1, cv=20,
            #                       min_features_to_select=i, n_jobs=-1, 
            #                       scoring='accuracy')
            #     selector = selector.fit(self.train, y_train)
            #     inds = selector.support_#list(range(21))
            #     self.meta_estimator.fit(self.train[:, inds], y_train)
            #     score = self.meta_estimator.score(self.train[:, inds], y_train)
            #     if score > best_score:
            #         self.meta_feature_inds = inds
            #         best_score = score
            # self.meta_estimator.fit(self.train[:, self.meta_feature_inds], y_train)
            # print(self.meta_feature_inds)
            self.meta_feature_inds = [True]*18
        self.meta_estimator.fit(self.train[:, self.meta_feature_inds], y_train)
        y_test_p = self.meta_estimator.predict(self.test[:, self.meta_feature_inds])
        return y_test_p

    
    def predict_proba(self, y_train):
        self.meta_estimator.fit(self.train, y_train)
        y_proba = self.meta_estimator.predict_proba(self.test)
        return y_proba